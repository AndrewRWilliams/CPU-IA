{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mbart-large-cc25-frquad-qg\")\n",
    "output = pipe(\"Créateur » (Maker), lui aussi au singulier, « <hl> le Suprême Berger <hl> » (The Great Shepherd) ; de l'autre, des réminiscences de la théologie de l'Antiquité : le tonnerre, voix de Jupiter, « Et souvent ta voix gronde en un tonnerre terrifiant », etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MT5(LightningModule):\n",
    "    \"\"\"\n",
    "    Google MT5 transformer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize module.\n",
    "\n",
    "        :param model_name_or_path: model name\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.save_hyperparameters()\n",
    "        self.model = pipeline(\"text2text-generation\", \"lmqg/mbart-large-cc25-frquad-qg\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "        #                                                use_fast=True) if model_name_or_path is not None else None\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Forward inputs.\n",
    "\n",
    "        :param inputs: dictionary of inputs (input_ids, attention_mask, labels)\n",
    "        \"\"\"\n",
    "\n",
    "        return self.model(**inputs)\n",
    "\n",
    "\n",
    "    def ae(self, batch: list[str], max_length: int = 512, **kwargs):\n",
    "        \"\"\"\n",
    "        Answer extraction prediction.\n",
    "\n",
    "        :param batch: list of context\n",
    "        :param max_length: max length of output\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform inputs\n",
    "        sentences = [f\"extract: {context}\" for context in batch]\n",
    "\n",
    "        # Predict\n",
    "        answers =[self.model(sentence)[0]['generated_text'] for sentence in sentences]\n",
    "\n",
    "        return answers\n",
    "\n",
    "\n",
    "    def qg(self, batch: list[str], max_length: int = 512, **kwargs):\n",
    "        \"\"\"\n",
    "        Question generation prediction.\n",
    "\n",
    "        :param batch: batch of context with highlighted elements\n",
    "        :param max_length: max length of output\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform inputs\n",
    "        sentences = [f\"generate: {context}\" for context in batch]\n",
    "\n",
    "        # Predict\n",
    "\n",
    "        questions = [self.model(sentence)[0]['generated_text'] for sentence in sentences]\n",
    "        \n",
    "        return questions\n",
    "\n",
    "\n",
    "    def qa(self, batch: list[dict], max_length: int = 512, **kwargs):\n",
    "        \"\"\"\n",
    "        Question answering prediction.\n",
    "\n",
    "        :param batch: batch of dict {question: q, context: c}\n",
    "        :param max_length: max length of output\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform inputs\n",
    "        sentences = [f\"question: {context['question']}  context: {context['context']}\" for context in batch]\n",
    "\n",
    "        # Predict\n",
    "        predictions = [self.model(sentence)[0]['generated_text'] for sentence in sentences]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def multitask(self, batch: list[str], max_length: int = 512, **kwargs):\n",
    "        \"\"\"\n",
    "        Answer extraction + question generation + question answering.\n",
    "\n",
    "        :param batch: list of context\n",
    "        :param max_length: max length of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        # Build output dict\n",
    "        dict_batch = {'context': [context for context in batch], 'answers': [], 'questions': [], 'answers_bis': []}\n",
    "\n",
    "        # Iterate over context\n",
    "        for context in batch:\n",
    "            answers = self.ae(batch=[context], max_length=max_length, **kwargs)[0]\n",
    "            answers = answers.split('<sep>')\n",
    "            answers = [ans.strip() for ans in answers if ans != ' ']\n",
    "            dict_batch['answers'].append(answers)\n",
    "            for_qg = [f\"{context.replace(ans, f'<hl> {ans} <hl> ')}\" for ans in answers]\n",
    "            questions = self.qg(batch=for_qg, max_length=max_length, **kwargs)\n",
    "            dict_batch['questions'].append(questions)\n",
    "            new_answers = self.qa([{'context': context, 'question': question} for question in questions],\n",
    "                                  max_length=max_length, **kwargs)\n",
    "            dict_batch['answers_bis'].append(new_answers)\n",
    "        return dict_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andrew Williams\\Documents\\github\\CPU-IA\\qa_generation.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Andrew%20Williams/Documents/github/CPU-IA/qa_generation.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mblocks\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m# coding:utf-8\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mFilename: mt5.py\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mAuthor: @DvdNss\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mCreated on 12/30/2021\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport os\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport nltk\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport gradio as gr\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport torch\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfrom typing import List\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfrom pytorch_lightning import LightningModule\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfrom transformers import MT5ForConditionalGeneration, AutoTokenizer, pipeline\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mclass MT5(LightningModule):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    Google MT5 transformer class.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def __init__(self, model_name_or_path: str = None):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Initialize module.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param model_name_or_path: model name\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        super().__init__()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Load model and tokenizer\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        self.save_hyperparameters()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        self.model = MT5ForConditionalGeneration.from_pretrained(\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            model_name_or_path) if model_name_or_path is not None else pipeline(\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext2text-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlmqg/mbart-large-cc25-frquad-qg\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m                                                       use_fast=True) if model_name_or_path is not None else None\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def forward(self, **inputs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Forward inputs.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param inputs: dictionary of inputs (input_ids, attention_mask, labels)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return self.model(**inputs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def ae(self, batch: List[str], max_length: int = 512, **kwargs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Answer extraction prediction.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param batch: list of context\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param max_length: max length of output\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Transform inputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        inputs = [f\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mextract: \u001b[39;49m\u001b[39m{context}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m for context in batch]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Predict\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        outputs = self.predict(inputs=inputs, max_length=max_length, **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def qg(self, batch: List[str] = None, max_length: int = 512, **kwargs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Question generation prediction.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param batch: batch of context with highlighted elements\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param max_length: max length of output\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Transform inputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        inputs = [f\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgenerate: \u001b[39;49m\u001b[39m{context}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m for context in batch]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Predict\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        outputs = self.predict(inputs=inputs, max_length=max_length, **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def qa(self, batch: List[dict], max_length: int = 512, **kwargs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Question answering prediction.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param batch: batch of dict \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39mquestion: q, context: c}\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param max_length: max length of output\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Transform inputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        inputs = [f\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mquestion: \u001b[39;49m\u001b[39m{context[\\'question\\']}\u001b[39;49;00m\u001b[39m  context: \u001b[39;49m\u001b[39m{context[\\'context\\']}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m for context in batch]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Predict\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        outputs = self.predict(inputs=inputs, max_length=max_length, **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def multitask(self, batch: List[str], max_length: int = 512, **kwargs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Answer extraction + question generation + question answering.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param batch: list of context\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param max_length: max length of outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Build output dict\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        dict_batch = \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mcontext\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: [context for context in batch], \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39manswers\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: [], \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mquestions\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: [], \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39manswers_bis\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: []}\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Iterate over context\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        for context in batch:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            answers = self.ae(batch=[context], max_length=max_length, **kwargs)[0]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            answers = answers.split(\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m<sep>\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            answers = [ans.strip() for ans in answers if ans != \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            dict_batch[\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39manswers\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m].append(answers)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            for_qg = [f\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39mcontext.replace(ans, f\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m<hl> \u001b[39;49m\u001b[39m{ans}\u001b[39;49;00m\u001b[39m <hl> \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)}\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m for ans in answers]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            questions = self.qg(batch=for_qg, max_length=max_length, **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            dict_batch[\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mquestions\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m].append(questions)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            new_answers = self.qa([\u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mcontext\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: context, \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mquestion\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m: question} for question in questions],\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m                                  max_length=max_length, **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m            dict_batch[\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39manswers_bis\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m].append(new_answers)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return dict_batch\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    def predict(self, inputs, max_length, **kwargs):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        Inference processing.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param inputs: list of inputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        :param max_length: max_length of outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Tokenize inputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        inputs = self.tokenizer(inputs, max_length=max_length, padding=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mmax_length\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m, truncation=True,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m                                return_tensors=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Retrieve input_ids and attention_mask\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        input_ids = inputs.input_ids.to(self.model.device)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        attention_mask = inputs.attention_mask.to(self.model.device)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Predict\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m                                      **kwargs)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        # Decode outputs\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        predictions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        return predictions\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# Load nltk punkt tokenizer\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mnltk.download(\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mpunkt\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mwith gr.Row():\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    with gr.Column():\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    # Define input\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        context = gr.Textbox(lines=5, label=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mContext\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    with gr.Column():\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        qa = gr.Textbox(lines=5, label=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mQA pair\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mgen_btn = gr.Button(label=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGenerate\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m, variant=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprimary\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mdef generate(context):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    model = MT5(model_name_or_path=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgoogle/mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    qa = model.multitask([context])\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    return context\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mgen_btn.click(\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    fn=generate,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    inputs=context,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    outputs=qa)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:2358\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2356\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2357\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2358\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2359\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\ipython_ext.py:16\u001b[0m, in \u001b[0;36mload_ipython_extension.<locals>.blocks\u001b[1;34m(line, cell, local_ns)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m@register_cell_magic\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[39m@needs_local_scope\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mblocks\u001b[39m(line, cell, local_ns\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m __demo\u001b[39m.\u001b[39mclear():\n\u001b[1;32m---> 16\u001b[0m         exec(cell, \u001b[39mNone\u001b[39;49;00m, local_ns)\n\u001b[0;32m     17\u001b[0m         __demo\u001b[39m.\u001b[39mlaunch(quiet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m<string>:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m<string>:50\u001b[0m, in \u001b[0;36mMT5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "%%blocks\n",
    "# coding:utf-8\n",
    "\"\"\"\n",
    "Filename: mt5.py\n",
    "Author: @DvdNss\n",
    "\n",
    "Created on 12/30/2021\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "from typing import list\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer, pipeline\n",
    "\n",
    "# Load nltk punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "with gr.Row():\n",
    "    with gr.Column():\n",
    "    # Define input\n",
    "        context = gr.Textbox(lines=5, label=\"Context\")\n",
    "\n",
    "    with gr.Column():\n",
    "        qa = gr.Textbox(lines=5, label=\"QA pair\")\n",
    "\n",
    "gen_btn = gr.Button(label=\"Generate\", variant=\"primary\")\n",
    "\n",
    "def generate(context):\n",
    "    model = MT5(model_name_or_path=\"google/mt5-small\")\n",
    "    qa = model.multitask([context])\n",
    "    return context\n",
    "\n",
    "gen_btn.click(\n",
    "    fn=generate,\n",
    "    inputs=context,\n",
    "    outputs=qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qui est le père de Fielding?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MT5(model_name_or_path=None)\n",
    "\n",
    "ctxt = str(\"Trainor wrote the song with Justin Weaver and Caitlyn Smith, and produced it with Chris Gelbuda. Epic Records released it as the album's fourth single on June 23, 2015.\")\n",
    "\n",
    "sentences = nltk.sent_tokenize(ctxt)\n",
    "\n",
    "out = model.ae([sentences[0]], max_length=32)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mbart-large-cc25-frquad-qg\")\n",
    "output = pipe(\"What if I change the context?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trainor wrote the song with Justin Weaver and Caitlyn Smith, and produced it with Chris Gelbuda.',\n",
       " \"Epic Records released it as the album's fourth single on June 23, 2015.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-frquad-qg-ae\")\n",
    "\n",
    "# answer extraction\n",
    "answer = pipe(\"extract answers: Créateur » (Maker), lui aussi au singulier, « <hl> le Suprême Berger <hl> » (The Great Shepherd) ; de l'autre, des réminiscences de la théologie de l'Antiquité : le tonnerre, voix de Jupiter, « Et souvent ta voix gronde en un tonnerre terrifiant », etc.\")\n",
    "\n",
    "# question generation\n",
    "question = pipe(\"generate question: Créateur » (Maker), lui aussi au singulier, « <hl> le Suprême Berger <hl> » (The Great Shepherd) ; de l'autre, des réminiscences de la théologie de l'Antiquité : le tonnerre, voix de Jupiter, « Et souvent ta voix gronde en un tonnerre terrifiant », etc.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Quel est le nom du personnage qui a écrit The Great Scottish ?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the OpenAI way, it's 2 cents for 750 words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bddb0f9e69f73f12f6855f6668e5f26ffce888f7babaa88d6009915ae26b172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
